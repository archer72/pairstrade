{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb566048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import tensorflow as tf\n",
    "#import tensorflow.keras as k\n",
    "#from tensorflow.keras import layers\n",
    "#import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d53e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "#util\n",
    "\n",
    "def get_data(symbols, dates_range, update):\n",
    "    import os\n",
    "    cwd = os.getcwd()\n",
    "    dir = cwd + \"/data/\"\n",
    "    df = pd.DataFrame()\n",
    "    symbols.append('spy')\n",
    "    for symbol in symbols:\n",
    "        print(\"symbol\", symbol)\n",
    "        filename = dir + symbol + '.csv'\n",
    "        data = pd.read_csv(filename, index_col=[\"Date\"], parse_dates=['Date'])\n",
    "        if df.empty:\n",
    "            if 'Close' in data:\n",
    "                df[symbol] = data['Close']\n",
    "            else:\n",
    "                df[symbol] = data['Rate']\n",
    "        else:\n",
    "            if 'Close' in data:\n",
    "                df[symbol] = data['Close']\n",
    "            else:\n",
    "                df[symbol] = data['Rate']\n",
    "    return df[df.index.isin(dates_range)].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581cc3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol scg\n",
      "symbol wec\n",
      "symbol interest_rates\n",
      "symbol vix\n",
      "symbol spy\n",
      "---BUY WITH longA greater/equal 0\n",
      "Buying 100 shares of scg at a price of $36.77 per share, for a total cost of $3677.0000000000005.\n",
      "Shorting 100 shares of wec at a price of $43.049999 per share, for a total cost of $4304.9999.\n",
      "Total cost is $7981.999900000001\n",
      "Pre-transaction cash is $100000\n",
      "Post-transaction cash is $92018.0001\n",
      "{'cash': 92018.0001, 'a_vol': [100], 'a_price': [36.77], 'b_vol': [100], 'b_price': [43.049999], 'longA': 1}\n",
      "{'cash': 92018.0001, 'a_vol': [100], 'a_price': [36.77], 'b_vol': [100], 'b_price': [43.049999], 'longA': 1}\n",
      "---END OF BUY WITH longA greater/equal 0\n",
      "port value 100000.0\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0']\n",
      "[-0.01460351659829906, -0.2725060827250608, -0.10570615290100782, 0.34484246870953816, 0.0]\n",
      "[-0.02568782985131124, -0.34549878345498786, -0.11474498587742288, 0.39145455330168333, 0.0]\n",
      "New date is\n",
      "2008-03-17 00:00:00\n",
      "Reward is 0.0\n",
      "---HOLDING\n",
      "{'cash': 92018.0001, 'a_vol': [100], 'a_price': [36.77], 'b_vol': [100], 'b_price': [43.049999], 'longA': 1}\n",
      "port value 99954.0001\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001']\n",
      "[-0.02568782985131124, -0.34549878345498786, -0.11474498587742288, 0.39145455330168333, -0.0004599989999999332]\n",
      "[-0.021105550000443896, -0.4744525547445255, -0.07796859549975954, 0.11307729823047041, -0.0004599989999999332]\n",
      "New date is\n",
      "2008-03-18 00:00:00\n",
      "Reward is -45.99989999999525\n",
      "---BUY WITH longA greater/equal 0\n",
      "Buying 100 shares of scg at a price of $37.16 per share, for a total cost of $3715.9999999999995.\n",
      "Shorting 100 shares of wec at a price of $43.810001 per share, for a total cost of $4381.0001.\n",
      "Total cost is $8097.000099999999\n",
      "Pre-transaction cash is $92018.0001\n",
      "Post-transaction cash is $83921.0\n",
      "{'cash': 83921.0, 'a_vol': [100, 100], 'a_price': [36.77, 37.16], 'b_vol': [100, 100], 'b_price': [43.049999, 43.810001], 'longA': 1}\n",
      "{'cash': 83921.0, 'a_vol': [100, 100], 'a_price': [36.77, 37.16], 'b_vol': [100, 100], 'b_price': [43.049999, 43.810001], 'longA': 1}\n",
      "---END OF BUY WITH longA greater/equal 0\n",
      "port value 99962.9998\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998']\n",
      "[-0.021105550000443896, -0.4744525547445255, -0.07796859549975954, 0.11307729823047041, -0.0003700019999999249]\n",
      "[-0.0197514934464319, -0.4939172749391728, -0.10080719454668019, 0.28787224859732397, -0.0003700019999999249]\n",
      "New date is\n",
      "2008-03-19 00:00:00\n",
      "Reward is 8.999700000000303\n",
      "---SELLING WITH longA > 0\n",
      "Selling our long investment of 100 shares for $36.73\n",
      "Return is $3672.9999999999995\n",
      "Cover our long investment of 100 shares that we bought for $43.049999 and add to it a gain of $-0.20000100000000032 (43.049999 - 43.25) for each of our 100 stocks.\n",
      "Short return is $4284.9998\n",
      "Old cash is $83921.0\n",
      "New cash is $91878.9998\n",
      "{'cash': 91878.9998, 'a_vol': [100], 'a_price': [37.16], 'b_vol': [100], 'b_price': [43.810001], 'longA': 1}\n",
      "Old portfolio value is $99962.9998\n",
      "New portfolio value is $99989.00000000001\n",
      "Reward is $26.0002000000095\n",
      "port value 99989.00000000001\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998', '2008-03-19,36.73,43.25,sell,-4.0000000000009095,-0.0010878433505577669,-20.00010000000003,-0.00464578408004145,-6.520000000000003,2.08,130.320007,29.84,99989.00000000001']\n",
      "[-0.0197514934464319, -0.4939172749391728, -0.10080719454668019, 0.28787224859732397, -0.00010999999999983245]\n",
      "[-0.023954221900540085, -0.45985401459854014, -0.08866343490405049, 0.14889948208890802, -0.00010999999999983245]\n",
      "New date is\n",
      "2008-03-20 00:00:00\n",
      "Reward is 26.0002000000095\n",
      "---SELLING WITH longA > 0\n",
      "Selling our long investment of 100 shares for $36.790001000000004\n",
      "Return is $3679.0001\n",
      "Cover our long investment of 100 shares that we bought for $43.810001 and add to it a gain of $0.2900009999999966 (43.810001 - 43.52) for each of our 100 stocks.\n",
      "Short return is $4410.0001999999995\n",
      "Old cash is $91878.9998\n",
      "New cash is $99968.0001\n",
      "{'cash': 99968.0001, 'a_vol': [], 'a_price': [], 'b_vol': [], 'b_price': [], 'longA': 0}\n",
      "Old portfolio value is $99989.00000000001\n",
      "New portfolio value is $99968.0001\n",
      "Reward is $-20.999900000009802\n",
      "port value 99968.0001\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998', '2008-03-19,36.73,43.25,sell,-4.0000000000009095,-0.0010878433505577669,-20.00010000000003,-0.00464578408004145,-6.520000000000003,2.08,130.320007,29.84,99989.00000000001', '2008-03-20,36.790001000000004,43.52,sell,-36.99989999999934,-0.00995691603875117,29.000099999999662,0.006619515941120308,-6.729998999999999,2.22,132.080002,26.620001000000002,99968.0001']\n",
      "[-0.023954221900540085, -0.45985401459854014, -0.08866343490405049, 0.14889948208890802, -0.0003199989999999042]\n",
      "[-0.016043769040590017, -0.4939172749391728, -0.07044775059086628, 0.11048769961156668, -0.0003199989999999042]\n",
      "New date is\n",
      "2008-03-24 00:00:00\n",
      "Reward is -20.999900000009802\n",
      "---HOLDING\n",
      "{'cash': 99968.0001, 'a_vol': [], 'a_price': [], 'b_vol': [], 'b_price': [], 'longA': 0}\n",
      "port value 99968.0001\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998', '2008-03-19,36.73,43.25,sell,-4.0000000000009095,-0.0010878433505577669,-20.00010000000003,-0.00464578408004145,-6.520000000000003,2.08,130.320007,29.84,99989.00000000001', '2008-03-20,36.790001000000004,43.52,sell,-36.99989999999934,-0.00995691603875117,29.000099999999662,0.006619515941120308,-6.729998999999999,2.22,132.080002,26.620001000000002,99968.0001', '2008-03-24,36.580002,42.900002,hold,0,0,0,0,-6.32,2.08,134.720001,25.73,99968.0001']\n",
      "[-0.016043769040590017, -0.4939172749391728, -0.07044775059086628, 0.11048769961156668, -0.0003199989999999042]\n",
      "[-0.022288146400644138, -0.4111922141119222, -0.06955073129686817, 0.1100560638757011, -0.0003199989999999042]\n",
      "New date is\n",
      "2008-03-25 00:00:00\n",
      "Reward is 0.0\n",
      "---HOLDING\n",
      "{'cash': 99968.0001, 'a_vol': [], 'a_price': [], 'b_vol': [], 'b_price': [], 'longA': 0}\n",
      "port value 99968.0001\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998', '2008-03-19,36.73,43.25,sell,-4.0000000000009095,-0.0010878433505577669,-20.00010000000003,-0.00464578408004145,-6.520000000000003,2.08,130.320007,29.84,99989.00000000001', '2008-03-20,36.790001000000004,43.52,sell,-36.99989999999934,-0.00995691603875117,29.000099999999662,0.006619515941120308,-6.729998999999999,2.22,132.080002,26.620001000000002,99968.0001', '2008-03-24,36.580002,42.900002,hold,0,0,0,0,-6.32,2.08,134.720001,25.73,99968.0001', '2008-03-25,36.59,43.209998999999996,hold,0,0,0,0,-6.619998999999993,2.42,134.850006,25.719998999999998,99968.0001']\n",
      "[-0.022288146400644138, -0.4111922141119222, -0.06955073129686817, 0.1100560638757011, -0.0003199989999999042]\n",
      "[-0.020474360916311385, -0.4403892944038931, -0.08093560040398262, 0.12559343979283533, -0.0003199989999999042]\n",
      "New date is\n",
      "2008-03-26 00:00:00\n",
      "Reward is 0.0\n",
      "---SELLING (ACTUALLY BUYING) WITH long <= 0\n",
      "Buying 100 shares of wec at a price of $43.25 per share, for a total cost of $4325.0.\n",
      "Shorting 100 shares of scg at a price of $36.700001 per share, for a total cost of $3670.0001.\n",
      "Total cost is $7995.0001\n",
      "Pre-transaction cash is $99968.0001\n",
      "Post-transaction cash is $91973.0\n",
      "{'cash': 91973.0, 'a_vol': [100], 'a_price': [36.700001], 'b_vol': [100], 'b_price': [43.25], 'longA': -1}\n",
      "{'cash': 91973.0, 'a_vol': [100], 'a_price': [36.700001], 'b_vol': [100], 'b_price': [43.25], 'longA': -1}\n",
      "---END OF SELLING (ACTUALLY BUYING) WITH long <= 0\n",
      "port value 99968.0001\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998', '2008-03-19,36.73,43.25,sell,-4.0000000000009095,-0.0010878433505577669,-20.00010000000003,-0.00464578408004145,-6.520000000000003,2.08,130.320007,29.84,99989.00000000001', '2008-03-20,36.790001000000004,43.52,sell,-36.99989999999934,-0.00995691603875117,29.000099999999662,0.006619515941120308,-6.729998999999999,2.22,132.080002,26.620001000000002,99968.0001', '2008-03-24,36.580002,42.900002,hold,0,0,0,0,-6.32,2.08,134.720001,25.73,99968.0001', '2008-03-25,36.59,43.209998999999996,hold,0,0,0,0,-6.619998999999993,2.42,134.850006,25.719998999999998,99968.0001', '2008-03-26,36.700001,43.25,sell,0,0,0,0,-6.549999,2.3,133.199997,26.08,99968.0001']\n",
      "[-0.020474360916311385, -0.4403892944038931, -0.08093560040398262, 0.12559343979283533, -0.0003199989999999042]\n",
      "[-0.0326586664341052, -0.44768856447688565, -0.08383353747902267, 0.11696154510142409, -0.0003199989999999042]\n",
      "New date is\n",
      "2008-03-27 00:00:00\n",
      "Reward is 0.0\n",
      "---SELLING (ACTUALLY BUYING) WITH long <= 0\n",
      "Buying 100 shares of wec at a price of $43.889998999999996 per share, for a total cost of $4388.9999.\n",
      "Shorting 100 shares of scg at a price of $36.75 per share, for a total cost of $3675.0.\n",
      "Total cost is $8063.9999\n",
      "Pre-transaction cash is $91973.0\n",
      "Post-transaction cash is $83909.0001\n",
      "{'cash': 83909.0001, 'a_vol': [100, 100], 'a_price': [36.700001, 36.75], 'b_vol': [100, 100], 'b_price': [43.25, 43.889998999999996], 'longA': -1}\n",
      "{'cash': 83909.0001, 'a_vol': [100, 100], 'a_price': [36.700001, 36.75], 'b_vol': [100, 100], 'b_price': [43.25, 43.889998999999996], 'longA': -1}\n",
      "---END OF SELLING (ACTUALLY BUYING) WITH long <= 0\n",
      "port value 100027.00010000002\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998', '2008-03-19,36.73,43.25,sell,-4.0000000000009095,-0.0010878433505577669,-20.00010000000003,-0.00464578408004145,-6.520000000000003,2.08,130.320007,29.84,99989.00000000001', '2008-03-20,36.790001000000004,43.52,sell,-36.99989999999934,-0.00995691603875117,29.000099999999662,0.006619515941120308,-6.729998999999999,2.22,132.080002,26.620001000000002,99968.0001', '2008-03-24,36.580002,42.900002,hold,0,0,0,0,-6.32,2.08,134.720001,25.73,99968.0001', '2008-03-25,36.59,43.209998999999996,hold,0,0,0,0,-6.619998999999993,2.42,134.850006,25.719998999999998,99968.0001', '2008-03-26,36.700001,43.25,sell,0,0,0,0,-6.549999,2.3,133.199997,26.08,99968.0001', '2008-03-27,36.75,43.889998999999996,sell,0,0,0,0,-7.139998999999996,2.27,132.779999,25.879998999999998,100027.00010000002']\n",
      "[-0.0326586664341052, -0.44768856447688565, -0.08383353747902267, 0.11696154510142409, 0.00027000100000029725]\n",
      "[-0.03960377206914134, -0.4914841849148419, -0.09259641653332584, 0.10962447129909347, 0.00027000100000029725]\n",
      "New date is\n",
      "2008-03-28 00:00:00\n",
      "Reward is 59.00000000001455\n",
      "---BUYING (ACTUALLY SELLING) WITH longA < 0\n",
      "Selling our long investment of 100 shares for $43.599998\n",
      "Long initial is 4325.0\n",
      "Long return is 4359.9998\n",
      "Long return - long initial = 34.999799999999595\n",
      "(long return - long initial) / long initial = 0.008092439306358288\n",
      "Return is $4359.9998\n",
      "Cover our long investment of 100 shares that we bought for $36.700001 and add to it a gain of $0.49000200000000405 (36.700001 - 36.209998999999996) for each of our 100 stocks.\n",
      "Short initial is 3670.0001\n",
      "Short return is 3719.0003000000006\n",
      "Absolute return for short is 49.000200000000405\n",
      "Percetn return for short is 0.013351552769712569\n",
      "Short return is $3719.0003000000006\n",
      "Old cash is $83909.0001\n",
      "New cash is $91988.00020000001\n",
      "{'cash': 91988.00020000001, 'a_vol': [100], 'a_price': [36.75], 'b_vol': [100], 'b_price': [43.889998999999996], 'longA': -1}\n",
      "Old portfolio value is $100027.00010000002\n",
      "New portfolio value is $100077.00010000002\n",
      "Reward is $50.0\n",
      "port value 100077.00010000002\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998', '2008-03-19,36.73,43.25,sell,-4.0000000000009095,-0.0010878433505577669,-20.00010000000003,-0.00464578408004145,-6.520000000000003,2.08,130.320007,29.84,99989.00000000001', '2008-03-20,36.790001000000004,43.52,sell,-36.99989999999934,-0.00995691603875117,29.000099999999662,0.006619515941120308,-6.729998999999999,2.22,132.080002,26.620001000000002,99968.0001', '2008-03-24,36.580002,42.900002,hold,0,0,0,0,-6.32,2.08,134.720001,25.73,99968.0001', '2008-03-25,36.59,43.209998999999996,hold,0,0,0,0,-6.619998999999993,2.42,134.850006,25.719998999999998,99968.0001', '2008-03-26,36.700001,43.25,sell,0,0,0,0,-6.549999,2.3,133.199997,26.08,99968.0001', '2008-03-27,36.75,43.889998999999996,sell,0,0,0,0,-7.139998999999996,2.27,132.779999,25.879998999999998,100027.00010000002', '2008-03-28,36.209998999999996,43.599998,buy,49.000200000000405,0.013351552769712569,34.999799999999595,0.008092439306358288,-7.389999000000003,2.09,131.509995,25.709999,100077.00010000002']\n",
      "[-0.03960377206914134, -0.4914841849148419, -0.09259641653332584, 0.10962447129909347, 0.0007700010000002422]\n",
      "[-0.03884711679798236, -0.3892944038929441, -0.08942242893781138, 0.10530863185153216, 0.0007700010000002422]\n",
      "New date is\n",
      "2008-03-31 00:00:00\n",
      "Reward is 50.0\n",
      "---BUYING (ACTUALLY SELLING) WITH longA < 0\n",
      "Selling our long investment of 100 shares for $43.990002000000004\n",
      "Long initial is 4388.9999\n",
      "Long return is 4399.0002\n",
      "Long return - long initial = 10.000300000000607\n",
      "(long return - long initial) / long initial = 0.002278491735668667\n",
      "Return is $4399.0002\n",
      "Cover our long investment of 100 shares that we bought for $36.75 and add to it a gain of $0.16999799999999965 (36.75 - 36.580002) for each of our 100 stocks.\n",
      "Short initial is 3675.0\n",
      "Short return is 3691.9998\n",
      "Absolute return for short is 16.999799999999965\n",
      "Percetn return for short is 0.004625795918367337\n",
      "Short return is $3691.9998\n",
      "Old cash is $91988.00020000001\n",
      "New cash is $100079.00020000001\n",
      "{'cash': 100079.00020000001, 'a_vol': [], 'a_price': [], 'b_vol': [], 'b_price': [], 'longA': 0}\n",
      "Old portfolio value is $100077.00010000002\n",
      "New portfolio value is $100079.00020000001\n",
      "Reward is $2.000099999990198\n",
      "port value 100079.00020000001\n",
      "['2008-03-14,36.77,43.049999,buy,0,0,0,0,-6.279998999999997,2.99,129.610001,31.16,100000.0', '2008-03-17,36.310001,43.049999,hold,0,0,0,0,-6.739998,2.69,128.300003,32.240002000000004,99954.0001', '2008-03-18,37.16,43.810001,buy,0,0,0,0,-6.650001000000003,2.16,133.630005,25.790001,99962.9998', '2008-03-19,36.73,43.25,sell,-4.0000000000009095,-0.0010878433505577669,-20.00010000000003,-0.00464578408004145,-6.520000000000003,2.08,130.320007,29.84,99989.00000000001', '2008-03-20,36.790001000000004,43.52,sell,-36.99989999999934,-0.00995691603875117,29.000099999999662,0.006619515941120308,-6.729998999999999,2.22,132.080002,26.620001000000002,99968.0001', '2008-03-24,36.580002,42.900002,hold,0,0,0,0,-6.32,2.08,134.720001,25.73,99968.0001', '2008-03-25,36.59,43.209998999999996,hold,0,0,0,0,-6.619998999999993,2.42,134.850006,25.719998999999998,99968.0001', '2008-03-26,36.700001,43.25,sell,0,0,0,0,-6.549999,2.3,133.199997,26.08,99968.0001', '2008-03-27,36.75,43.889998999999996,sell,0,0,0,0,-7.139998999999996,2.27,132.779999,25.879998999999998,100027.00010000002', '2008-03-28,36.209998999999996,43.599998,buy,49.000200000000405,0.013351552769712569,34.999799999999595,0.008092439306358288,-7.389999000000003,2.09,131.509995,25.709999,100077.00010000002', '2008-03-31,36.580002,43.990002000000004,buy,16.999799999999965,0.004625795918367337,10.000300000000607,0.002278491735668667,-7.410000000000004,2.51,131.970001,25.610001,100079.00020000001']\n",
      "[-0.03884711679798236, -0.3892944038929441, -0.08942242893781138, 0.10530863185153216, 0.0007900020000000119]\n",
      "[-0.036838713484300456, -0.4209245742092458, -0.05740697165423847, -0.02114803625377648, 0.0007900020000000119]\n",
      "New date is\n",
      "2008-04-01 00:00:00\n",
      "Reward is 2.000099999990198\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "'''\n",
    "# a simulator for wrapping the learner into a time frame of data features\n",
    "'''\n",
    "class Simulator(object):\n",
    "\n",
    "    def __init__(self, symbols,\n",
    "        start_date=dt.datetime(2008,1,1),                           #CHANGE DATES TO ACTUAL RANGE!!!\n",
    "        end_date= dt.datetime(2009,1,1)):\n",
    "\n",
    "        # frame a time period as world\n",
    "        self.dates_range = pd.date_range(start_date, end_date)\n",
    "\n",
    "        # initialize cash holdings\n",
    "        init_cash = 100000\n",
    "\n",
    "        #for visualization\n",
    "        self.data_out = []\n",
    "\n",
    "        # preprocessing time series\n",
    "        # stock symbol data\n",
    "        stock_symbols = symbols[:]\n",
    "        symbols.append('interest_rates')\n",
    "        symbols.append('vix')\n",
    "        # price data\n",
    "        prices_all = get_data(symbols, self.dates_range, True)\n",
    "\n",
    "        self.stock_A = stock_symbols[0]\n",
    "        self.stock_B = stock_symbols[1]\n",
    "\n",
    "        \"\"\"\n",
    "        #unemployment rate\n",
    "        temp_unemp = {}\n",
    "        unemployment = {}\n",
    "        with open('unemployment.csv') as unemp_file:\n",
    "            for line in csv.reader(unemp_file, delimiter=','):\n",
    "                curr_date = dt.strptime(line[0], '%B-%y')\n",
    "                temp_unemp[curr_date] = line[1]\n",
    "        for d in prices_all.keys():\n",
    "            temp_date = dt.datetime(d.year, d.month)\n",
    "            if temp_date in temp_unemp:\n",
    "                unemployment[d] = temp_unemp[temp_date]\n",
    "        \"\"\"\n",
    "\n",
    "        # first trading day\n",
    "        self.dateIdx = 0\n",
    "        self.date = prices_all.index[0]\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.prices = prices_all[stock_symbols]\n",
    "        self.prices_SPY = prices_all['spy']\n",
    "        self.prices_VIX = prices_all['vix']\n",
    "        self.prices_interest_rate = prices_all['interest_rates']\n",
    "\n",
    "        # keep track of portfolio value as a series\n",
    "        self.portfolio = {'cash': init_cash, 'a_vol': [], 'a_price': [], 'b_vol': [], 'b_price': [], 'longA': 0}\n",
    "        self.port_val = self.port_value_for_output()\n",
    "\n",
    "        # hardcode enumerating of features\n",
    "        \"\"\"\n",
    "        self.sma = SMA(self.dates_range)\n",
    "        self.bbp = BBP(self.dates_range)\n",
    "        self.rsi = RSI(self.dates_range)\n",
    "        \"\"\"\n",
    "\n",
    "    def init_state(self, lookback=50):\n",
    "        \"\"\"\n",
    "        return init states of the market\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for _ in range(lookback):\n",
    "            states.append(self.get_state(self.date))\n",
    "            self.dateIdx += 1\n",
    "            self.date = self.prices.index[self.dateIdx]\n",
    "\n",
    "        return states\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Code indirectly based on previous code written by group member Yiding\n",
    "        Zhao for the Machine Learning for Trading course\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Take an action, and move the date forward,\n",
    "        record the reward of the action and date\n",
    "        action: buy, sell, hold\n",
    "        return (reward, market status)\n",
    "        \"\"\"\n",
    "        # change state accordingly\n",
    "\n",
    "        buy_volume = 100\n",
    "        abs_return_A = 0\n",
    "        pct_return_A = 0\n",
    "        abs_return_B = 0\n",
    "        pct_return_B = 0\n",
    "\n",
    "        if (action == 'buy'):\n",
    "            if (self.portfolio['longA'] >= 0):\n",
    "\n",
    "                if verbose: print('---BUY WITH longA greater/equal 0')\n",
    "                long_cost = buy_volume * self.prices.loc[self.date, self.stock_A]\n",
    "\n",
    "                if verbose: print('Buying ' + str(buy_volume) + ' shares of ' + self.stock_A + ' at a price of $' + str(self.prices.loc[self.date, self.stock_A]) + ' per share, for a total cost of $' + str(long_cost) + '.')\n",
    "                short_cost = buy_volume * self.prices.loc[self.date, self.stock_B]\n",
    "\n",
    "                if verbose: print('Shorting ' + str(buy_volume) + ' shares of ' + self.stock_B + ' at a price of $' + str(self.prices.loc[self.date, self.stock_B]) + ' per share, for a total cost of $' + str(short_cost) + '.')\n",
    "                total_cost = short_cost + long_cost\n",
    "\n",
    "                if verbose: print('Total cost is $' + str(total_cost))\n",
    "\n",
    "                if verbose: print('Pre-transaction cash is $' + str(self.portfolio['cash']))\n",
    "                self.portfolio['cash'] -= total_cost\n",
    "\n",
    "                if verbose: print('Post-transaction cash is $' + str(self.portfolio['cash']))\n",
    "\n",
    "                self.portfolio['a_vol'].append(buy_volume)\n",
    "                self.portfolio['a_price'].append(self.prices.loc[self.date, self.stock_A])\n",
    "                self.portfolio['b_vol'].append(buy_volume)\n",
    "                self.portfolio['b_price'].append(self.prices.loc[self.date, self.stock_B])\n",
    "                self.portfolio['longA'] = 1\n",
    "\n",
    "                if verbose: print(self.portfolio)\n",
    "                old_port_val = self.port_val\n",
    "                self.port_val = self.port_value_for_output()\n",
    "\n",
    "                if verbose: print(self.portfolio)\n",
    "                reward = self.port_val - old_port_val\n",
    "                if verbose: print('---END OF BUY WITH longA greater/equal 0')\n",
    "\n",
    "            else: #longA < 0 --> sell in reverse\n",
    "                if verbose: print('---BUYING (ACTUALLY SELLING) WITH longA < 0')\n",
    "                if verbose: print('Selling our long investment of ' + str(self.portfolio['b_vol'][0]) + ' shares for $' + str(self.prices.loc[self.date, self.stock_B]))\n",
    "                long_initial = self.portfolio['b_vol'][0] * self.portfolio['b_price'][0]\n",
    "                long_return = self.portfolio['b_vol'].pop(0) * self.prices.loc[self.date, self.stock_B]\n",
    "                abs_return_B = long_return - long_initial\n",
    "                pct_return_B = float(abs_return_B) / long_initial\n",
    "                if verbose:print('Long initial is ' + str(long_initial))\n",
    "                if verbose:print('Long return is ' + str(long_return))\n",
    "                if verbose:print('Long return - long initial = ' + str(abs_return_B))\n",
    "                if verbose:print('(long return - long initial) / long initial = ' + str(pct_return_B))\n",
    "                self.portfolio['b_price'].pop(0)\n",
    "                if verbose: print('Return is $' + str(long_return))\n",
    "                if verbose: print('Cover our long investment of ' + str(self.portfolio['a_vol'][0]) + ' shares that we bought for $' + str(self.portfolio['a_price'][0]) + ' and add to it a gain of $' + str((self.portfolio['a_price'][0] - self.prices.loc[self.date, self.stock_A])) + ' (' + str((self.portfolio['a_price'][0])) + ' - ' + str(self.prices.loc[self.date, self.stock_A]) + ') for each of our ' + str(self.portfolio['a_vol'][0]) + ' stocks.')\n",
    "                short_initial = self.portfolio['a_vol'][0] * self.portfolio['a_price'][0]\n",
    "                abs_return_A = (self.portfolio['a_vol'][0] * (self.portfolio['a_price'][0] - self.prices.loc[self.date, self.stock_A]))\n",
    "                short_return = self.portfolio['a_vol'][0] * self.portfolio['a_price'][0]\n",
    "                short_return += (self.portfolio['a_vol'].pop(0) * (self.portfolio['a_price'].pop(0) - self.prices.loc[self.date, self.stock_A]))\n",
    "                pct_return_A = float(abs_return_A) / short_initial\n",
    "                if verbose:print('Short initial is ' + str(short_initial))\n",
    "                if verbose:print('Short return is ' + str(short_return))\n",
    "                if verbose:print('Absolute return for short is ' + str(abs_return_A))\n",
    "                if verbose:print('Percetn return for short is ' + str(pct_return_A))\n",
    "                if verbose: print('Short return is $' + str(short_return))\n",
    "                if verbose: print('Old cash is $' + str(self.portfolio['cash']))\n",
    "                new_cash = self.portfolio['cash'] + long_return + short_return\n",
    "                self.portfolio['cash'] = new_cash\n",
    "                if verbose: print('New cash is $' + str(self.portfolio['cash']))\n",
    "                self.portfolio['longA'] = -1 if (len(self.portfolio['a_vol']) > 0) else 0\n",
    "                old_port_val = self.port_val\n",
    "                self.port_val = self.port_value_for_output()\n",
    "                if verbose: print(self.portfolio)\n",
    "                reward = self.port_val - old_port_val\n",
    "                if verbose: print('Old portfolio value is $' + str(old_port_val))\n",
    "                if verbose: print('New portfolio value is $' + str(self.port_val))\n",
    "                if verbose: print('Reward is $' + str(reward))\n",
    "\n",
    "        elif (action == 'sell'):\n",
    "            if (self.portfolio['longA'] > 0):\n",
    "                if verbose: print('---SELLING WITH longA > 0')\n",
    "                if verbose: print('Selling our long investment of ' + str(self.portfolio['a_vol'][0]) + ' shares for $' + str(self.prices.loc[self.date, self.stock_A]))\n",
    "                long_initial = self.portfolio['a_vol'][0] * self.portfolio['a_price'][0]\n",
    "                long_return = self.portfolio['a_vol'].pop(0) * self.prices.loc[self.date, self.stock_A]\n",
    "                abs_return_A = long_return - long_initial\n",
    "                pct_return_A = float(abs_return_A) / long_initial\n",
    "                self.portfolio['a_price'].pop(0)\n",
    "                if verbose: print('Return is $' + str(long_return))\n",
    "                if verbose: print('Cover our long investment of ' + str(self.portfolio['b_vol'][0]) + ' shares that we bought for $' + str(self.portfolio['b_price'][0]) + ' and add to it a gain of $' + str((self.portfolio['b_price'][0] - self.prices.loc[self.date, self.stock_B])) + ' (' + str((self.portfolio['b_price'][0])) + ' - ' + str(self.prices.loc[self.date, self.stock_B]) + ') for each of our ' + str(self.portfolio['b_vol'][0]) + ' stocks.')\n",
    "                short_initial = self.portfolio['b_vol'][0] * self.portfolio['b_price'][0]\n",
    "                abs_return_B = (self.portfolio['b_vol'][0] * (self.portfolio['b_price'][0] - self.prices.loc[self.date, self.stock_B]))\n",
    "                short_return = self.portfolio['b_vol'][0] * self.portfolio['b_price'][0]\n",
    "                short_return += (self.portfolio['b_vol'].pop(0) * (self.portfolio['b_price'].pop(0) - self.prices.loc[self.date, self.stock_B]))\n",
    "                pct_return_B = float(abs_return_B) / short_initial\n",
    "                if verbose: print('Short return is $' + str(short_return))\n",
    "                if verbose: print('Old cash is $' + str(self.portfolio['cash']))\n",
    "                new_cash = self.portfolio['cash'] + long_return + short_return\n",
    "                self.portfolio['cash'] = new_cash\n",
    "                if verbose: print('New cash is $' + str(self.portfolio['cash']))\n",
    "                self.portfolio['longA'] = 1 if (len(self.portfolio['a_vol']) > 0) else 0\n",
    "                old_port_val = self.port_val\n",
    "                self.port_val = self.port_value_for_output()\n",
    "                if verbose: print(self.portfolio)\n",
    "                reward = self.port_val - old_port_val\n",
    "                if verbose: print('Old portfolio value is $' + str(old_port_val))\n",
    "                if verbose: print('New portfolio value is $' + str(self.port_val))\n",
    "                if verbose: print('Reward is $' + str(reward))\n",
    "            else: # longA <= 0 --> buy in reverse\n",
    "                if verbose: print('---SELLING (ACTUALLY BUYING) WITH long <= 0')\n",
    "                long_cost = buy_volume * self.prices.loc[self.date, self.stock_B]\n",
    "                if verbose: print('Buying ' + str(buy_volume) + ' shares of ' + self.stock_B + ' at a price of $' + str(self.prices.loc[self.date, self.stock_B]) + ' per share, for a total cost of $' + str(long_cost) + '.')\n",
    "                short_cost = buy_volume * self.prices.loc[self.date, self.stock_A]\n",
    "                if verbose: print('Shorting ' + str(buy_volume) + ' shares of ' + self.stock_A + ' at a price of $' + str(self.prices.loc[self.date, self.stock_A]) + ' per share, for a total cost of $' + str(short_cost) + '.')\n",
    "                total_cost = short_cost + long_cost\n",
    "                if verbose: print('Total cost is $' + str(total_cost))\n",
    "                if verbose: print('Pre-transaction cash is $' + str(self.portfolio['cash']))\n",
    "                self.portfolio['cash'] -= total_cost\n",
    "                if verbose: print('Post-transaction cash is $' + str(self.portfolio['cash']))\n",
    "                self.portfolio['a_vol'].append(buy_volume)\n",
    "                self.portfolio['a_price'].append(self.prices.loc[self.date, self.stock_A])\n",
    "                self.portfolio['b_vol'].append(buy_volume)\n",
    "                self.portfolio['b_price'].append(self.prices.loc[self.date, self.stock_B])\n",
    "                self.portfolio['longA'] = -1\n",
    "                if verbose: print(self.portfolio)\n",
    "                old_port_val = self.port_val\n",
    "                self.port_val = self.port_value_for_output()\n",
    "                if verbose: print(self.portfolio)\n",
    "                reward = self.port_val - old_port_val\n",
    "                if verbose: print('---END OF SELLING (ACTUALLY BUYING) WITH long <= 0')\n",
    "        else: #hold\n",
    "            if verbose: print('---HOLDING')\n",
    "            old_port_val = self.port_val\n",
    "            self.port_val = self.port_value_for_output()\n",
    "            if verbose: print(self.portfolio)\n",
    "            reward = self.port_val - old_port_val\n",
    "        #self.port_val = self.port_value_for_output()\n",
    "        print(\"port value\", self.port_val)\n",
    "        self.data_out.append(self.date.isoformat()[0:10] + ',' + str(self.prices.loc[self.date, self.stock_A]) + ',' + str(self.prices.loc[self.date, self.stock_B]) + ',' + action + ',' + str(abs_return_A) + ',' +  str(pct_return_A) + ',' + str(abs_return_B) + ',' + str(pct_return_B) + ',' + str((self.prices.loc[self.date, self.stock_A] - self.prices.loc[self.date, self.stock_B])) + ',' + str(self.prices_interest_rate[self.date]) + ',' + str(self.prices_SPY[self.date]) + ',' + str(self.prices_VIX[self.date]) + ',' + str(self.port_val))\n",
    "        if verbose: print(self.data_out)\n",
    "        state = self.get_state(self.date)\n",
    "        if verbose: print(state)\n",
    "        self.dateIdx += 1\n",
    "        if self.dateIdx < len(self.prices.index):\n",
    "            self.date = self.prices.index[self.dateIdx]\n",
    "        if verbose: print(self.get_state(self.date))\n",
    "        if verbose: print('New date is')\n",
    "        if verbose: print(self.date)\n",
    "        if verbose: print('Reward is ' + str(reward))\n",
    "        return (reward, state)\n",
    "\n",
    "    def get_state(self, date):\n",
    "        \"\"\"\n",
    "        return state of the market, i.e. prices of certain symbols,\n",
    "        number of shares hold\n",
    "        \"\"\"\n",
    "        if date not in self.dates_range:\n",
    "            if verbose: print('Date was out of bounds.')\n",
    "            if verbose: print(date)\n",
    "            exit\n",
    "\n",
    "        # a vector of features\n",
    "        if (date == self.prices.index[-1]):\n",
    "            file_name = \"data_for_vis_%s.csv\" % dt.datetime.now().strftime(\"%H-%M-%S\")\n",
    "            print (\"saving to\", file_name)\n",
    "            file = open(file_name, 'w');\n",
    "            for line in self.data_out:\n",
    "                file.write(line);\n",
    "                file.write('\\n')\n",
    "            file.close()\n",
    "        return [self.prices.loc[date, self.stock_A]/self.prices.loc[self.prices.index[0], self.stock_A] - self.prices.loc[date, self.stock_B]/self.prices.loc[self.prices.index[0], self.stock_B],\n",
    "            self.prices_interest_rate[date]/self.prices_interest_rate[0] - 1,\n",
    "            self.prices_SPY[date]/self.prices_SPY[0] - 1,\n",
    "            self.prices_VIX[date]/self.prices_VIX[0] - 1,\n",
    "            self.port_val / 100000.0 - 1,\n",
    "            ]\n",
    "\n",
    "    # calculate the current value of cash and stock holdings\n",
    "    def port_value(self):\n",
    "        value = self.portfolio['cash']\n",
    "        if (len(self.portfolio['a_vol']) > 0):\n",
    "            for i in range(len(self.portfolio['a_vol'])):\n",
    "                value += (self.portfolio['a_vol'][i] * self.portfolio['a_price'][i])\n",
    "        if (len(self.portfolio['b_vol']) > 0):\n",
    "            for i in range(len(self.portfolio['b_vol'])):\n",
    "                value += (self.portfolio['b_vol'][i] * self.portfolio['b_price'][i])\n",
    "        return value\n",
    "\n",
    "    # alternate calculation of the current value of cash and stock holdings\n",
    "    def port_value_for_output(self):\n",
    "        value = self.portfolio['cash']\n",
    "        if (self.portfolio['longA'] > 0):\n",
    "            value += (sum(self.portfolio['a_vol']) * self.prices.loc[self.date, self.stock_A])\n",
    "            for i in range(len(self.portfolio['b_vol'])):\n",
    "                value += (self.portfolio['b_vol'][i] * self.portfolio['b_price'][i])\n",
    "                value += (self.portfolio['b_vol'][i] * (self.portfolio['b_price'][i] - self.prices.loc[self.date, self.stock_B]))\n",
    "        if (self.portfolio['longA'] < 0):\n",
    "            value += (sum(self.portfolio['b_vol']) * self.prices.loc[self.date, self.stock_B])\n",
    "            for i in range(len(self.portfolio['a_vol'])):\n",
    "                value += (self.portfolio['a_vol'][i] * self.portfolio['a_price'][i])\n",
    "                value += (self.portfolio['a_vol'][i] * (self.portfolio['a_price'][i] - self.prices.loc[self.date, self.stock_A]))\n",
    "        return value\n",
    "\n",
    "    def has_more(self):\n",
    "        if ((self.dateIdx < len(self.prices.index)) == False):\n",
    "            print('\\n\\n\\n*****')\n",
    "            print(self.baseline())\n",
    "            print('*****\\n\\n\\n')\n",
    "        return self.dateIdx < len(self.prices.index)\n",
    "\n",
    "    def baseline(self):\n",
    "        num_shares = 100000 / self.prices_SPY[0]\n",
    "        return num_shares * self.prices_SPY[-1]\n",
    "\n",
    "def main():\n",
    "    # TO DO\n",
    "    sim = Simulator(['scg', 'wec'])\n",
    "    sim.init_state()\n",
    "    #sim.get_state(sim.prices.index[-1])\n",
    "    sim.step('buy')\n",
    "    sim.step('hold')\n",
    "    sim.step('buy')\n",
    "    sim.step('sell')\n",
    "    sim.step('sell')\n",
    "    sim.step('hold')\n",
    "    sim.step('hold')\n",
    "    sim.step('sell')\n",
    "    sim.step('sell')\n",
    "    sim.step('buy')\n",
    "    sim.step('buy')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8eb017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3b0054",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_5148/3954194794.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_5148/3954194794.py\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    layers.Dense(3, activation = 'softmax')\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ef14854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "class PolicyGradientAgent(object):\n",
    "    def __init__(self, n_batch=5, batch_size=64, update_cycle=3, update_target_cycle= 3, lookback=[]):\n",
    "        #self.n_bath = 1 # number of batches to run in each update\n",
    "        self.batch_size = batch_size # number of x, y pairs to fit model\n",
    "        self.update_cycle = update_cycle # cyle to update actor and critics model, i.e. every 5 days\n",
    "        self.update_target_cycle = update_target_cycle #cycle for update target model\n",
    "        self.lookback_size = len(lookback)\n",
    "        self.n_feature = len(lookback[0])\n",
    "        self.states = lookback[:]\n",
    "        self.choices = []\n",
    "        self.rewards = []\n",
    "        self.model = LSTM(self.lookback_size, len(lookback[0]))\n",
    "\n",
    "        self.size_of_replay = self.batch_size*4\n",
    "\n",
    "        #initialize critic network\n",
    "        self.critic_agent = CriticsAgent(batch_size = self.batch_size, critic_update_cycle=self.update_cycle, target_update_cycle = self.update_target_cycle, lookback = lookback)\n",
    "\n",
    "    def reset(self, lookback):\n",
    "        # clear all rewards and actions\n",
    "        self.lookback_size = len(lookback)\n",
    "        self.states = lookback[:]\n",
    "        self.choices = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def init_query(self):\n",
    "        probs = self.model.predict(self.states)\n",
    "        print (\"probabilities\", probs)\n",
    "        self.action = np.random.choice(np.arange(3), p=probs)\n",
    "        return self.action\n",
    "\n",
    "    def query(self, new_state, reward):\n",
    "\n",
    "\n",
    "        # update critics's experience repaly\n",
    "        self.critic_agent.update_history(self.states, self.rewards)\n",
    "\n",
    "\n",
    "        self.choices.append(self.action)\n",
    "        self.rewards.append(reward)\n",
    "        # for i in range(len(self.rewards) - 2, -1, -1):\n",
    "        #     reward *= 0.95\n",
    "        #     self.rewards[i] += reward\n",
    "\n",
    "\n",
    "        if len(self.choices) % self.update_cycle == 0 and len(self.choices) >= self.batch_size * 1.25:\n",
    "            #update actor, choose random sample\n",
    "            random_sample = np.random.choice(np.arange(max(0,len(self.rewards) - self.size_of_replay - 1), len(self.rewards) - 1), self.batch_size)\n",
    "            print (\"updating actor\")\n",
    "            self.update(random_sample)\n",
    "            #update critic\n",
    "            print (\"updating crititc\")\n",
    "            self.critic_agent.update_critic(random_sample)\n",
    "\n",
    "        if len(self.choices) % self.update_target_cycle == 0 and len(self.choices) >= self.batch_size * 1.25:\n",
    "            #update target network\n",
    "            print (\"updating target\")\n",
    "            self.critic_agent.update_target()\n",
    "\n",
    "\n",
    "        print (\"new state\", new_state)\n",
    "        self.states.append(new_state)\n",
    "        probs = self.model.predict(self.states[-self.lookback_size:])\n",
    "\n",
    "        print (\"probabilities\", probs)\n",
    "        if np.isnan(probs[0]):\n",
    "            sys.exit()\n",
    "        self.action = np.random.choice(3, p=probs)\n",
    "\n",
    "        # update critics's experience repaly\n",
    "        self.critic_agent.update_history(self.states, self.rewards)\n",
    "\n",
    "        return self.action\n",
    "\n",
    "    def update(self, random_sample):\n",
    "        # for j in range(min(self.n_bath, len(self.rewards)/self.batch_size)):\n",
    "        #     offset = j * self.batch_size\n",
    "        #     Input = []\n",
    "        #     if not offset:\n",
    "        #         Input.append(self.states[- self.lookback_size -offset:])\n",
    "        #     else:\n",
    "        #         Input.append(self.states[- self.lookback_size - offset: -offset])\n",
    "        #     for i in range(1, self.batch_size):\n",
    "        #         Input.append(self.states[- i - self.lookback_size - offset: - i - offset])\n",
    "        #     Input = np.array(Input).astype(theano.config.floatX)\n",
    "\n",
    "        #     rewards = self.rewards[-self.batch_size - offset: -offset] if offset else self.rewards[-self.batch_size:]\n",
    "        #     rewards = np.array(rewards).astype(theano.config.floatX)\n",
    "\n",
    "        #     choices = self.choices[-self.batch_size - offset: -offset] if offset else self.choices[-self.batch_size:]\n",
    "        #     choices = np.array(choices).astype(np.int32)\n",
    "        #     self.model.train(Input, choices, rewards)\n",
    "\n",
    "\n",
    "        Input = np.zeros((self.batch_size, self.lookback_size, self.n_feature), dtype = theano.config.floatX)\n",
    "        Input_next = np.zeros((self.batch_size, self.lookback_size, self.n_feature), dtype = theano.config.floatX)\n",
    "        choices = np.array(self.choices, dtype = np.int32)[random_sample]\n",
    "        # td_advantage = np.zeros(self.batch_size, dtype = np.int32)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            random_id = random_sample[i]\n",
    "            state_cur = np.array(self.states[random_id: random_id+self.lookback_size], dtype = theano.config.floatX)\n",
    "            state_next = np.array(self.states[random_id+1: random_id+self.lookback_size+1], dtype = theano.config.floatX)\n",
    "\n",
    "            Input[i,:,:] = state_cur\n",
    "            Input_next[i,:,:] = state_next\n",
    "            #td error estimating the advantage function, to multiply policy gradient\n",
    "        td_advantage = np.array(self.rewards)[random_sample] + 0.95*self.critic_agent.target_batch_query(Input_next) - self.critic_agent.batch_query(Input)\n",
    "        self.model.train(Input, choices, td_advantage)\n",
    "\n",
    "\n",
    "#from tensorflow.keras.layers import Dense\n",
    "#from tensorflow.keras.layers import LSTM as lstm\n",
    "#from tensorflow.keras import Model, Sequential\n",
    "#from keras import backend as K\n",
    "#from tensorflow.keras.optimizers import Adagrad, Adam\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import LSTM as lstm\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "\n",
    "class model(object):\n",
    "    def __init__(self, seq_len, n_feature):\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.n_feature = n_feature\n",
    "        self.model = self.buildnetwork()\n",
    "\n",
    "\n",
    "    def buildnetwork(self):\n",
    "        model = Sequential()\n",
    "        model.add(lstm(20, dropout=0.2,input_shape = (self.seq_len, self.n_feature)))\n",
    "        model.add(Dense(1, activation=None))\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adagrad(lr=0.002,clipvalue=10), metrics=['mean_squared_error'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def predict(self, x):\n",
    "        # x is a tensor having shape (1, seq_len, n_feature)\n",
    "        return self.model.predict(x)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def train(self, x_batch, target_batch):\n",
    "        # x_batch is a tensor having shape (batch_size, seq_len, n_feature)\n",
    "        # target batch is a tensor having shape (batch_size, 1)\n",
    "        self.model.train_on_batch(x_batch, target_batch)\n",
    "\n",
    "class target_model(model):\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        #weights should be returned by \"get_weights\" method in Critic network\n",
    "        self.model.set_weights(weights)\n",
    "\n",
    "\n",
    "class CriticsAgent(object):\n",
    "    def __init__(self, batch_size=100, critic_update_cycle=2, target_update_cycle=5,lookback=[]):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.states = lookback[:]\n",
    "        self.rewards = []\n",
    "\n",
    "        self.n_feature = len(lookback[0])\n",
    "        self.lookback_size = len(lookback)\n",
    "\n",
    "        self.critic_model = model(self.lookback_size, len(lookback[0]))\n",
    "        self.target_model = target_model(self.lookback_size, len(lookback[0]))\n",
    "\n",
    "        self.critic_update_cycle = critic_update_cycle\n",
    "        self.target_update_cycle = target_update_cycle\n",
    "\n",
    "        self.gamma = 0.95\n",
    "\n",
    "        self.count = 0\n",
    "\n",
    "        self.init_lookback = lookback\n",
    "\n",
    "    def update_critic(self, random_sample):\n",
    "        #random_sample = np.random.choice(np.arange(len(self.rewards)-1), self.batch_size)\n",
    "\n",
    "        states_batch = np.zeros((self.batch_size, self.lookback_size, self.n_feature), dtype = \"float32\")\n",
    "        states_next_batch = np.zeros((self.batch_size, self.lookback_size, self.n_feature),dtype = \"float32\")\n",
    "\n",
    "        #print random_sample\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            random_id = random_sample[i]\n",
    "            states_batch[i,:,:] =np.array(self.states[random_id:random_id+self.lookback_size]).astype(\"float32\")\n",
    "            states_next_batch[i,:,:] =np.array(self.states[random_id + 1:(random_id+self.lookback_size +1)]).astype(\"float32\")\n",
    "\n",
    "        reward_batch = np.array([self.rewards[i] for i in random_sample]).astype(\"float32\")\n",
    "        #using target model to predict\n",
    "        target_value = self.target_model.predict(states_next_batch).flatten()*self.gamma + reward_batch\n",
    "\n",
    "        self.critic_model.train(states_batch, target_value.reshape(self.batch_size,1))\n",
    "\n",
    "\n",
    "    def query(self, state):\n",
    "        #query state for one day (shape: (lookback_size, n_feature))\n",
    "        value = self.critic_model.predict(state.reshape(1,self.lookback_size, self.n_feature))[0][0]\n",
    "        #print \"predicted_value:\", value\n",
    "        return value\n",
    "\n",
    "    def batch_query(self, states):\n",
    "        values = self.critic_model.predict(states.reshape(self.batch_size, self.lookback_size, self.n_feature))[0]\n",
    "        return values\n",
    "\n",
    "\n",
    "    def target_batch_query(self, states):\n",
    "        values = self.target_model.predict(states.reshape(self.batch_size, self.lookback_size, self.n_feature))[0]\n",
    "        return values\n",
    "\n",
    "    def update_target(self):\n",
    "        weights = self.critic_model.get_weights()\n",
    "        self.target_model.set_weights(weights)\n",
    "\n",
    "\n",
    "    def update_history(self, states, rewards):\n",
    "        self.states = states\n",
    "        self.rewards = rewards\n",
    "\n",
    "    # def test(self):\n",
    "    #     data = np.array(self.init_lookback).reshape(1,200,5).astype(\"float32\")\n",
    "    #     print self.critic_model.predict(data)\n",
    "    #     self.update_target()\n",
    "    #     print self.target_model.predict(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0f4eb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class LSTM(object):\n",
    "    def __init__(self, seq_len, n_feature, n_actions = 3):\n",
    "        self.Inputs = Input(shape=(seq_len, n_feature), dtype=tf.float32)\n",
    "        self.buildNetwork()\n",
    "        \n",
    "        #self.q = tf.placeholder(shape=None, dtype=tf.float32) \n",
    "        self.q = Input(shape=(1,), dtype=tf.float32)\n",
    "        #self.actions = tf.placeholder(shape=(None, n_actions), dtype=tf.float32)\n",
    "        \n",
    "        #self.actions_prob = K.softmax(self.last_layer)\n",
    "        self.actions_prob = self.last_layer\n",
    "        self.output_fn = Model(inputs = self.Inputs, outputs = self.last_layer)\n",
    "        self.output_fn.summary()\n",
    "        \n",
    "        choices = tf.placeholder(shape=(1,), dtype=tf.int32)\n",
    "        \n",
    "        px = self.actions_prob[K.arange(self.actions_prob.shape[1]), choices]\n",
    "        print(px, px.shape)\n",
    "        ###self.log_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.last_layer, labels=self.actions)\n",
    "        log_px = tf.math.log(px)\n",
    "        print(log_px, log_px.shape)\n",
    "        print(self.q, self.q.shape)\n",
    "        #sys.exit()\n",
    "        ###self.cost = tf.reduce_mean(self.q * self.log_policy) \n",
    "        cost = layers.dot([-self.q, log_px], axes = 1)\n",
    "        print(cost)\n",
    "        \n",
    "        self.train_fn = Model(inputs = [self.Inputs, choices, self.q], outputs = [cost, px, log_px])\n",
    "        self.optimizer = tf.train.AdagradOptimizer(learning_rate=0.0008).minimize(cost)\n",
    "        #self.train_fn.compile(optimizer = Adagrad(learning_rate = 0.0008), loss = cost, metrics = 'accuracy')\n",
    "    \n",
    "    \n",
    "    def buildNetwork(self):\n",
    "        inputs = self.Inputs\n",
    "        x = lstm(20, dropout= 0.2)(self.Inputs)\n",
    "        self.last_layer = Dense(3, activation = 'softmax')(x)\n",
    "        \n",
    "        return self.last_layer\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.array([x]).astype('float32')\n",
    "        prediction = self.output_fn(x)\n",
    "        return prediction[0]\n",
    "\n",
    "    def train(self, Input, choices, rewards):\n",
    "        if rewards.std() != 0:\n",
    "            rewards = (rewards - rewards.mean()) / rewards.std()\n",
    "        cost, px, log_px = self.train_fn(Input, choices, rewards)\n",
    "        if np.isnan(cost) or any(np.isnan(log_px)):\n",
    "            print (rewards, \"\\n\", cost, px, log_px)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951d243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c084beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    def __init__(self, seq_len, n_feature, n_actions = 3):\n",
    "        self.Inputs = k.Input(shape=(seq_len, n_feature), dtype=tf.float32)\n",
    "        self.buildNetwork()\n",
    "        \n",
    "        self.q = tf.placeholder(shape=(1,), dtype=tf.float32, name='pg_q') \n",
    "        self.actions = tf.placeholder(shape=(None, n_actions), dtype=tf.float32, name='pg_actions')\n",
    "        choices = tf.placeholder(shape=None, dtype=tf.int32)\n",
    "                \n",
    "        ###_layer = self.states\n",
    "        #for l in hidden_layers_size:\n",
    "        #    _layer = tf.layers.dense(inputs=_layer, units=l, activation=tf.nn.relu,\n",
    "        #                             kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        #self.last_layer = tf.layers.dense(inputs=_layer, units=num_of_actions, activation=None, # Linear activation\n",
    "        ###                                  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.action_prob = tf.nn.softmax(self.last_layer)\n",
    "        self.log_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.last_layer, labels=self.actions)\n",
    "        self.cost = tf.reduce_mean(self.q * self.log_policy) \n",
    "        self.optimizer = tf.train.AdagradOptimizer(learning_rate=0.0008).minimize(self.cost)\n",
    "        px = self.action_prob\n",
    "        log_px = self.log_policy\n",
    "        cost = self.cost\n",
    "        \n",
    "        \n",
    "        #self.train_fn = Model.compile(optimizer = Adagrad(learning_rate = 0.0008), loss = self.cost, metrics = 'accuracy')\n",
    "        self.train_fn = Model(inputs = [self.Inputs, choices, self.q], outputs = [cost, px, log_px])\n",
    "        \n",
    "    \n",
    "    \n",
    "    def buildNetwork(self):\n",
    "        x = lstm(20, dropout= 0.2, input_shape = (self.seq_len, self.n_feature))(self.Inputs)\n",
    "        self.last_layer = Dense(3, activation = None)(x)\n",
    "        \n",
    "        return self.last_layer\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.array([x]).astype('float32')\n",
    "        prediction = self.output_fn(x)\n",
    "        return prediction[0]\n",
    "\n",
    "    def train(self, Input, choices, rewards):\n",
    "        if rewards.std() != 0:\n",
    "            rewards = (rewards - rewards.mean()) / rewards.std()\n",
    "        cost, px, log_px = self.train_fn(Input, choices, rewards)\n",
    "        if np.isnan(cost) or any(np.isnan(log_px)):\n",
    "            print (rewards, \"\\n\", cost, px, log_px)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4acbc048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2f1d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras\n",
    "class LSTM(object):\n",
    "    def __init__(self, seq_len, n_feature, n_actions = 3):\n",
    "        self.Inputs = Input(shape=(seq_len, n_feature), dtype='float32')\n",
    "        self.buildNetwork()\n",
    "        \n",
    "        #self.q = tf.placeholder(shape=None, dtype=tf.float32) \n",
    "        self.q = Input(shape=(1,), dtype='float32')\n",
    "        #self.actions = tf.placeholder(shape=(None, n_actions), dtype=tf.float32)\n",
    "        \n",
    "        #self.actions_prob = tf.nn.softmax(self.last_layer)\n",
    "        self.actions_prob = self.last_layer\n",
    "        self.output_fn = Model(inputs = self.Inputs, outputs = self.actions_prob)\n",
    "        self.output_fn.summary()\n",
    "        \n",
    "        choices = Input(shape=(1,), dtype='int32')\n",
    "        \n",
    "        px = self.actions_prob[np.arange(self.actions_prob.shape[0].value), choices]\n",
    "        print(px, px.shape)\n",
    "        ###self.log_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.last_layer, labels=self.actions)\n",
    "        log_px = tf.math.log(px)\n",
    "        print(log_px, log_px.shape)\n",
    "        print(self.q, self.q.shape)\n",
    "        #sys.exit()\n",
    "        ###self.cost = tf.reduce_mean(self.q * self.log_policy) \n",
    "        cost = K.dot(-self.q, log_px)\n",
    "        print(cost)\n",
    "        self.train_fn = Model(inputs = [self.Inputs, choices, self.q], outputs = [cost, px, log_px])\n",
    "        self.optimizer = tf.train.AdagradOptimizer(learning_rate=0.0008).minimize(cost)\n",
    "        #self.train_fn.compile(optimizer = Adagrad(learning_rate = 0.0008), loss = cost, metrics = 'accuracy')\n",
    "    \n",
    "    \n",
    "    def buildNetwork(self):\n",
    "        inputs = self.Inputs\n",
    "        x = lstm(20, dropout= 0.2)(self.Inputs)\n",
    "        self.last_layer = Dense(3, activation = 'softmax')(x)\n",
    "        \n",
    "        return self.last_layer\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.array([x]).astype('float32')\n",
    "        prediction = self.output_fn(x)\n",
    "        return prediction[0]\n",
    "\n",
    "    def train(self, Input, choices, rewards):\n",
    "        if rewards.std() != 0:\n",
    "            rewards = (rewards - rewards.mean()) / rewards.std()\n",
    "        cost, px, log_px = self.train_fn(Input, choices, rewards)\n",
    "        if np.isnan(cost) or any(np.isnan(log_px)):\n",
    "            print (rewards, \"\\n\", cost, px, log_px)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b26aae2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol scg\n",
      "symbol wec\n",
      "symbol interest_rates\n",
      "symbol vix\n",
      "symbol spy\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_65 (InputLayer)        (None, 50, 5)             0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 20)                2080      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 2,143\n",
      "Trainable params: 2,143\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor 'arange_9:0' shape=(3,) dtype=int32>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38428/3202028481.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m '''\n\u001b[0;32m     30\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38428/3202028481.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0menv_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wec'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2002\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolicyGradientAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlookback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#critic_agent = CriticsAgent(lookback=env.init_state())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38428/2240201072.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_batch, batch_size, update_cycle, update_target_cycle, lookback)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookback_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlookback\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_of_replay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38428/2499161241.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, seq_len, n_feature, n_actions)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mchoices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mpx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions_prob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions_prob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m###self.log_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.last_layer, labels=self.actions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pairstr\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    764\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m       \u001b[0m_check_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m       \u001b[0mend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pairstr\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[1;34m(idx)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[1;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[1;31m# will break `_slice_helper` contract.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", got {!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor 'arange_9:0' shape=(3,) dtype=int32>"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "def main():\n",
    "    actions = [\"buy\", \"sell\", \"hold\"]\n",
    "\n",
    "    n_iter = 5\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        env_train = Simulator(['scg', 'wec'], dt.datetime(2002, 1, 4), dt.datetime(2016, 12, 31))\n",
    "\n",
    "        agent = PolicyGradientAgent(lookback=env_train.init_state())\n",
    "        #critic_agent = CriticsAgent(lookback=env.init_state())\n",
    "        action = agent.init_query()\n",
    "\n",
    "\n",
    "        while env_train.has_more():\n",
    "        \taction = actions[action] # map action from id to name\n",
    "        \tprint (\"Runner: Taking action\", env_train.date, action)\n",
    "        \treward, state = env_train.step(action)\n",
    "        \taction = agent.query(state, reward)\n",
    "'''\n",
    "    env_test = Simulator(['scg', 'wec'], dt.datetime(2013, 12, 30), dt.datetime(2016, 11, 30))\n",
    "    agent.reset(lookback=env_test.init_state())\n",
    "    while env_test.has_more():\n",
    "        action = actions[action] # map action from id to name\n",
    "        print \"Runner: Taking action\", env_test.date, action\n",
    "        reward, state = env_test.step(action)\n",
    "        action = agent.query(state, reward)\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808aa943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pairstr\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.policies import mlplstmpolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cb044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
